{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00df7fe7",
   "metadata": {},
   "source": [
    "# This note Book is for Models Traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4214ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbdbf3",
   "metadata": {},
   "source": [
    "### Checking that our GPU is available here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016e723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA is available.\n",
      "CUDA version: 12.4\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Tensor operation result: tensor([5., 7., 9.])\n",
      "PyTorch is working correctly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "try:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not installed.\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Test a simple tensor operation\n",
    "try:\n",
    "    x = torch.tensor([1.0, 2.0, 3.0])\n",
    "    y = torch.tensor([4.0, 5.0, 6.0])\n",
    "    print(\"Tensor operation result:\", x + y)\n",
    "    print(\"PyTorch is working correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5206c",
   "metadata": {},
   "source": [
    "### Data Transformation Techniques to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f570c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SheepVOCDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory containing images.\n",
    "            annotations_dir (str): Directory containing XML annotations in Pascal VOC format.\n",
    "            transforms (callable, optional): Transforms to be applied on the images.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Class-to-ID mapping\n",
    "        self.class_name_to_id = {\n",
    "            \"graze\": 1,\n",
    "            \"sit\": 2,\n",
    "            \"walk\": 3,\n",
    "            \"run\": 4\n",
    "        }\n",
    "\n",
    "        # Collect images and annotations\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.image_files.sort()\n",
    "\n",
    "        self.annotation_files = [f for f in os.listdir(self.annotations_dir) if f.lower().endswith('.xml')]\n",
    "        self.annotation_files.sort()\n",
    "\n",
    "        # Match images and annotations\n",
    "        image_basenames = set(os.path.splitext(img)[0] for img in self.image_files)\n",
    "        annot_basenames = set(os.path.splitext(ann)[0] for ann in self.annotation_files)\n",
    "        common = image_basenames.intersection(annot_basenames)\n",
    "        self.image_files = [img for img in self.image_files if os.path.splitext(img)[0] in common]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def parse_voc_xml(self, xml_path):\n",
    "        \"\"\"\n",
    "        Parses a Pascal VOC XML annotation file.\n",
    "\n",
    "        Args:\n",
    "            xml_path (str): Path to the XML annotation file.\n",
    "\n",
    "        Returns:\n",
    "            boxes (list): List of bounding boxes [xmin, ymin, xmax, ymax].\n",
    "            labels (list): List of corresponding class labels.\n",
    "        \"\"\"\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            if name not in self.class_name_to_id:\n",
    "                continue\n",
    "\n",
    "            label = self.class_name_to_id[name]\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = float(bndbox.find('xmin').text)\n",
    "            ymin = float(bndbox.find('ymin').text)\n",
    "            xmax = float(bndbox.find('xmax').text)\n",
    "            ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label)\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            img (Tensor): Transformed image.\n",
    "            target (dict): Target dictionary containing:\n",
    "                - boxes (Tensor): Bounding boxes.\n",
    "                - labels (Tensor): Class labels.\n",
    "                - image_id (Tensor): Image ID.\n",
    "                - area (Tensor): Areas of bounding boxes.\n",
    "                - iscrowd (Tensor): Crowd flags (all set to 0 for single objects).\n",
    "        \"\"\"\n",
    "        img_name = self.image_files[idx]\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        annot_path = os.path.join(self.annotations_dir, base_name + \".xml\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes, labels = self.parse_voc_xml(annot_path)\n",
    "\n",
    "        # Handle cases with no annotations\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Generate additional metadata\n",
    "        image_id = torch.tensor([idx])\n",
    "        if boxes.shape[0] > 0:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply transformations if defined\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de57ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224])\n",
      "Target: {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'image_id': tensor([0]), 'area': tensor([]), 'iscrowd': tensor([], dtype=torch.int64)}\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "# Paths\n",
    "images_dir = \"processed_dataset/train_images\"\n",
    "annotations_dir = \"processed_dataset/train_annotations\"\n",
    "\n",
    "# Define transforms\n",
    "transforms = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = SheepVOCDataset(images_dir, annotations_dir, transforms=transforms)\n",
    "\n",
    "# Example: Fetch a sample\n",
    "image, target = dataset[0]\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d361177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# def visualize_dataset_samples(dataset, class_name_to_id, num_images=9):\n",
    "#     \"\"\"\n",
    "#     Visualizes multiple images from the dataset with bounding boxes and labels.\n",
    "\n",
    "#     Args:\n",
    "#         dataset: The dataset object to sample images from.\n",
    "#         class_name_to_id (dict): Mapping of class names to IDs.\n",
    "#         num_images: Number of images to visualize (default: 9).\n",
    "#     \"\"\"\n",
    "#     # Reverse the class_name_to_id mapping\n",
    "#     id_to_class_name = {v: k for k, v in class_name_to_id.items()}\n",
    "\n",
    "#     # Define the grid size (rows and columns)\n",
    "#     cols = 3\n",
    "#     rows = num_images // cols if num_images % cols == 0 else (num_images // cols) + 1\n",
    "\n",
    "#     # Create the figure\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "#     axes = axes.flatten()\n",
    "\n",
    "#     for i in range(num_images):\n",
    "#         # Get the image and target\n",
    "#         image, target = dataset[i]\n",
    "\n",
    "#         # Convert the tensor to a PIL image\n",
    "#         image = to_pil_image(image)\n",
    "\n",
    "#         # Plot the image\n",
    "#         ax = axes[i]\n",
    "#         ax.imshow(image)\n",
    "#         ax.axis('off')\n",
    "\n",
    "#         # Plot bounding boxes and labels\n",
    "#         for box, label in zip(target['boxes'], target['labels']):\n",
    "#             x_min, y_min, x_max, y_max = box.tolist()\n",
    "#             width = x_max - x_min\n",
    "#             height = y_max - y_min\n",
    "\n",
    "#             # Add the bounding box\n",
    "#             rect = patches.Rectangle(\n",
    "#                 (x_min, y_min), width, height,\n",
    "#                 linewidth=2, edgecolor='red', facecolor='none'\n",
    "#             )\n",
    "#             ax.add_patch(rect)\n",
    "\n",
    "#             # Add the label with the class name\n",
    "#             class_name = id_to_class_name[label.item()]\n",
    "#             ax.text(\n",
    "#                 x_min, y_min - 5, f\"{class_name}\",\n",
    "#                 color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.7)\n",
    "#             )\n",
    "\n",
    "#     # Hide unused axes if num_images < rows * cols\n",
    "#     for j in range(num_images, len(axes)):\n",
    "#         axes[j].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# class_name_to_id = {\n",
    "#     \"graze\": 1,\n",
    "#     \"sit\": 2,\n",
    "#     \"walk\": 3,\n",
    "#     \"run\": 4\n",
    "# }\n",
    "\n",
    "# visualize_dataset_samples(dataset, class_name_to_id, num_images=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6072221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224])\n",
      "Target: {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'image_id': tensor([0]), 'area': tensor([]), 'iscrowd': tensor([], dtype=torch.int64)}\n"
     ]
    }
   ],
   "source": [
    "# Debugging the dataset\n",
    "image, target = dataset[0]  # Load a sample from the dataset\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24c07095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 203\n",
      "Example annotations: ['sheep101.xml', 'sheep76.xml', 'sheep150.xml', 'sheep201.xml', 'sheep115.xml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "annotations_dir = \"archive/annotations\"\n",
    "annotations = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
    "print(f\"Number of annotations: {len(annotations)}\")\n",
    "print(\"Example annotations:\", annotations[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e394a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object name: sheep\n",
      "Bounding box: xmin=23.0, ymin=26.0, xmax=235.0, ymax=258.0\n",
      "Object name: sheep\n",
      "Bounding box: xmin=286.0, ymin=69.0, xmax=390.0, ymax=211.0\n",
      "Boxes: [[23.0, 26.0, 235.0, 258.0], [286.0, 69.0, 390.0, 211.0]]\n",
      "Labels: ['sheep', 'sheep']\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_voc_xml_debug(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text.lower().strip()\n",
    "        print(f\"Object name: {name}\")\n",
    "        \n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "        print(f\"Bounding box: xmin={xmin}, ymin={ymin}, xmax={xmax}, ymax={ymax}\")\n",
    "\n",
    "        # Collect bounding boxes and labels\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(name)\n",
    "\n",
    "    return boxes, labels\n",
    "\n",
    "# Test with a specific annotation file\n",
    "xml_path = \"archive/annotations/sheep18.xml\"\n",
    "boxes, labels = parse_voc_xml_debug(xml_path)\n",
    "print(\"Boxes:\", boxes)\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96e0e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying Augmentations: 100%|██████████| 203/203 [00:09<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images directory: processed_dataset/train_images\n",
      "Train annotations directory: processed_dataset/train_annotations\n",
      "Validation images directory: processed_dataset/val_images\n",
      "Validation annotations directory: processed_dataset/val_annotations\n",
      "Number of training images: 812\n",
      "Number of training annotations: 812\n",
      "Number of validation images: 203\n",
      "Number of validation annotations: 203\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "images_path = \"archive/images\"  # Path to images\n",
    "annotations_path = \"archive/annotations\"  # Path to annotations\n",
    "output_path = \"processed_dataset\"\n",
    "\n",
    "# Directories for augmented data and final train/val splits\n",
    "augmented_images_dir = os.path.join(output_path, 'augmented_images')\n",
    "os.makedirs(augmented_images_dir, exist_ok=True)\n",
    "\n",
    "train_images_dir = os.path.join(output_path, 'train_images')\n",
    "train_annotations_dir = os.path.join(output_path, 'train_annotations')\n",
    "val_images_dir = os.path.join(output_path, 'val_images')\n",
    "val_annotations_dir = os.path.join(output_path, 'val_annotations')\n",
    "\n",
    "os.makedirs(train_images_dir, exist_ok=True)\n",
    "os.makedirs(train_annotations_dir, exist_ok=True)\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_annotations_dir, exist_ok=True)\n",
    "\n",
    "# Get all image file names\n",
    "image_files = [f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))]\n",
    "\n",
    "# Define data augmentation transforms\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply augmentations to all images\n",
    "for img_file in tqdm(image_files, desc=\"Applying Augmentations\"):\n",
    "    img_path = os.path.join(images_path, img_file)\n",
    "    annotation_file = os.path.splitext(img_file)[0] + '.xml'\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    for i in range(5):  # Generate 5 augmented images per original\n",
    "        augmented_img = augmentations(img)\n",
    "        augmented_img_pil = transforms.ToPILImage()(augmented_img)\n",
    "        augmented_img_name = f\"{os.path.splitext(img_file)[0]}_aug_{i}.jpeg\"\n",
    "        augmented_img_pil.save(os.path.join(augmented_images_dir, augmented_img_name))\n",
    "\n",
    "        # Copy corresponding annotation file if it exists\n",
    "        if os.path.exists(os.path.join(annotations_path, annotation_file)):\n",
    "            shutil.copy(\n",
    "                os.path.join(annotations_path, annotation_file),\n",
    "                os.path.join(augmented_images_dir, os.path.splitext(augmented_img_name)[0] + '.xml')\n",
    "            )\n",
    "\n",
    "# Split augmented data into training and validation sets\n",
    "augmented_image_files = [f for f in os.listdir(augmented_images_dir) if f.endswith('.jpeg')]\n",
    "train_files, val_files = train_test_split(augmented_image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move files to respective directories\n",
    "for file in train_files:\n",
    "    shutil.copy(os.path.join(augmented_images_dir, file), os.path.join(train_images_dir, file))\n",
    "    annotation_file = os.path.splitext(file)[0] + '.xml'\n",
    "    if os.path.exists(os.path.join(augmented_images_dir, annotation_file)):\n",
    "        shutil.copy(os.path.join(augmented_images_dir, annotation_file), os.path.join(train_annotations_dir, annotation_file))\n",
    "\n",
    "for file in val_files:\n",
    "    shutil.copy(os.path.join(augmented_images_dir, file), os.path.join(val_images_dir, file))\n",
    "    annotation_file = os.path.splitext(file)[0] + '.xml'\n",
    "    if os.path.exists(os.path.join(augmented_images_dir, annotation_file)):\n",
    "        shutil.copy(os.path.join(augmented_images_dir, annotation_file), os.path.join(val_annotations_dir, annotation_file))\n",
    "\n",
    "# Output summary\n",
    "print(\"Train images directory:\", train_images_dir)\n",
    "print(\"Train annotations directory:\", train_annotations_dir)\n",
    "print(\"Validation images directory:\", val_images_dir)\n",
    "print(\"Validation annotations directory:\", val_annotations_dir)\n",
    "print(\"Number of training images:\", len(os.listdir(train_images_dir)))\n",
    "print(\"Number of training annotations:\", len(os.listdir(train_annotations_dir)))\n",
    "print(\"Number of validation images:\", len(os.listdir(val_images_dir)))\n",
    "print(\"Number of validation annotations:\", len(os.listdir(val_annotations_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1dc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7901001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images_dir = \"processed_dataset/augmented_images\"\n",
    "train_annotations_dir = \"processed_dataset/train_annotations\"\n",
    "\n",
    "val_images_dir = \"processed_dataset/val_images\"\n",
    "val_annotations_dir = \"processed_dataset/val_annotations\"\n",
    "\n",
    "\n",
    " \n",
    "train_transforms = T.Compose([T.ToTensor()])\n",
    "val_transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "train_dataset = SheepVOCDataset(train_images_dir, train_annotations_dir, transforms=train_transforms)\n",
    "val_dataset = SheepVOCDataset(val_images_dir, val_annotations_dir, transforms=val_transforms)\n",
    "\n",
    " \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f1e24",
   "metadata": {},
   "source": [
    "# Loading/Training  FASTER-Rcnn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381bad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zahoor-ai-developer/Desktop/Project#3/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zahoor-ai-developer/Desktop/Project#3/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/zahoor-ai-developer/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "def get_model(num_classes):\n",
    "    \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "num_classes = 5\n",
    "model = get_model(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "958025c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b3e73",
   "metadata": {},
   "source": [
    "# Traning (This will take at least 10 hours to run) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b611d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/2], Step [0/406], Loss: 0.0022\n",
      "Epoch [0/2], Step [10/406], Loss: 0.0023\n",
      "Epoch [0/2], Step [20/406], Loss: 0.0009\n",
      "Epoch [0/2], Step [30/406], Loss: 0.0019\n",
      "Epoch [0/2], Step [40/406], Loss: 0.0036\n",
      "Epoch [0/2], Step [50/406], Loss: 0.0017\n",
      "Epoch [0/2], Step [60/406], Loss: 0.0031\n",
      "Epoch [0/2], Step [70/406], Loss: 0.0010\n",
      "Epoch [0/2], Step [80/406], Loss: 0.0008\n",
      "Epoch [0/2], Step [90/406], Loss: 0.0018\n",
      "Epoch [0/2], Step [100/406], Loss: 0.0022\n",
      "Epoch [0/2], Step [110/406], Loss: 0.0014\n",
      "Epoch [0/2], Step [120/406], Loss: 0.0012\n",
      "Epoch [0/2], Step [130/406], Loss: 0.0018\n",
      "Epoch [0/2], Step [140/406], Loss: 0.0000\n",
      "Epoch [0/2], Step [150/406], Loss: 0.0014\n",
      "Epoch [0/2], Step [160/406], Loss: 0.0006\n",
      "Epoch [0/2], Step [170/406], Loss: 0.0015\n",
      "Epoch [0/2], Step [180/406], Loss: 0.0012\n",
      "Epoch [0/2], Step [190/406], Loss: 0.0010\n",
      "Epoch [0/2], Step [200/406], Loss: 0.0015\n",
      "Epoch [0/2], Step [210/406], Loss: 0.0011\n",
      "Epoch [0/2], Step [220/406], Loss: 0.0015\n",
      "Epoch [0/2], Step [230/406], Loss: 0.0015\n",
      "Epoch [0/2], Step [240/406], Loss: 0.0024\n",
      "Epoch [0/2], Step [250/406], Loss: 0.0015\n",
      "Epoch [0/2], Step [260/406], Loss: 0.0009\n",
      "Epoch [0/2], Step [270/406], Loss: 0.0009\n",
      "Epoch [0/2], Step [280/406], Loss: 0.0018\n",
      "Epoch [0/2], Step [290/406], Loss: 0.0007\n",
      "Epoch [0/2], Step [300/406], Loss: 0.0009\n",
      "Epoch [0/2], Step [310/406], Loss: 0.0011\n",
      "Epoch [0/2], Step [320/406], Loss: 0.0007\n",
      "Epoch [0/2], Step [330/406], Loss: 0.0007\n",
      "Epoch [0/2], Step [340/406], Loss: 0.0007\n",
      "Epoch [0/2], Step [350/406], Loss: 0.0016\n",
      "Epoch [0/2], Step [360/406], Loss: 0.0019\n",
      "Epoch [0/2], Step [370/406], Loss: 0.0012\n",
      "Epoch [0/2], Step [380/406], Loss: 0.0024\n",
      "Epoch [0/2], Step [390/406], Loss: 0.0014\n",
      "Epoch [0/2], Step [400/406], Loss: 0.0017\n",
      "Epoch [1/2], Step [0/406], Loss: 0.0025\n",
      "Epoch [1/2], Step [10/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [20/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [30/406], Loss: 0.0003\n",
      "Epoch [1/2], Step [40/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [50/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [60/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [70/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [80/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [90/406], Loss: 0.0008\n",
      "Epoch [1/2], Step [100/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [110/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [120/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [130/406], Loss: 0.0019\n",
      "Epoch [1/2], Step [140/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [150/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [160/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [170/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [180/406], Loss: 0.0003\n",
      "Epoch [1/2], Step [190/406], Loss: 0.0008\n",
      "Epoch [1/2], Step [200/406], Loss: 0.0005\n",
      "Epoch [1/2], Step [210/406], Loss: 0.0013\n",
      "Epoch [1/2], Step [220/406], Loss: 0.0018\n",
      "Epoch [1/2], Step [230/406], Loss: 0.0000\n",
      "Epoch [1/2], Step [240/406], Loss: 0.0005\n",
      "Epoch [1/2], Step [250/406], Loss: 0.0017\n",
      "Epoch [1/2], Step [260/406], Loss: 0.0013\n",
      "Epoch [1/2], Step [270/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [280/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [290/406], Loss: 0.0007\n",
      "Epoch [1/2], Step [300/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [310/406], Loss: 0.0008\n",
      "Epoch [1/2], Step [320/406], Loss: 0.0018\n",
      "Epoch [1/2], Step [330/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [340/406], Loss: 0.0002\n",
      "Epoch [1/2], Step [350/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [360/406], Loss: 0.0015\n",
      "Epoch [1/2], Step [370/406], Loss: 0.0017\n",
      "Epoch [1/2], Step [380/406], Loss: 0.0010\n",
      "Epoch [1/2], Step [390/406], Loss: 0.0012\n",
      "Epoch [1/2], Step [400/406], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {losses.item():.4f}\")\n",
    "        i += 1\n",
    "\n",
    "   \n",
    "    lr_scheduler.step()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6323ca",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a86306e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward0>)}, {'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "images, targets = next(iter(val_loader))\n",
    "images = list(img.to(device) for img in images)\n",
    "outputs = model(images)\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68799d",
   "metadata": {},
   "source": [
    "# Loading/Training  Yolov8  (This will take at least 15-20 hours to Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define dataset paths\n",
    "train_images_dir = \"processed_dataset/augmented_images\"\n",
    "train_annotations_dir = \"processed_dataset/train_annotations\"\n",
    "\n",
    "val_images_dir = \"processed_dataset/val_images\"\n",
    "val_annotations_dir = \"processed_dataset/val_annotations\"\n",
    "\n",
    "# Create YOLOv8 dataset class (in YOLO format)\n",
    "class YOLODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = sorted(os.listdir(img_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        ann_path = os.path.join(self.ann_dir, img_file.replace('.png', '.txt'))\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read annotation\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(ann_path):\n",
    "            with open(ann_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    labels.append(int(parts[0]))  # label is first column\n",
    "                    boxes.append([float(x) for x in parts[1:]])  # bbox: x_center, y_center, width, height\n",
    "        \n",
    "        # Convert to tensor and normalize if transforms are provided\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        # YOLO expects: [img, boxes, labels]\n",
    "        return img, {\"boxes\": torch.tensor(boxes), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "# Define transforms\n",
    "train_transforms = T.Compose([T.ToTensor()])\n",
    "val_transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = YOLODataset(train_images_dir, train_annotations_dir, transforms=train_transforms)\n",
    "val_dataset = YOLODataset(val_images_dir, val_annotations_dir, transforms=val_transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Train YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Load YOLOv8 pre-trained model (you can replace 'yolov8n.pt' with a custom model)\n",
    "\n",
    "# Configure dataset paths (for YOLOv8 format)\n",
    "data_dict = {\n",
    "    'train': str(Path(train_images_dir).parent),\n",
    "    'val': str(Path(val_images_dir).parent),\n",
    "    'nc': 1,  # number of classes\n",
    "    'names': ['sheep']  # class names\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model.train(data=data_dict, epochs=50, batch_size=2, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5f07c",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, targets = next(iter(val_loader))\n",
    "images = list(img.to(device) for img in images)\n",
    "outputs = model(images)\n",
    "\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
